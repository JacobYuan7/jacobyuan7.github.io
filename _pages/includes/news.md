# ğŸ”¥ News
<!-- åŠ ç‚¹è¡¨æƒ…åŒ…,ç›´æ¥å¤åˆ¶å›¾ç‰‡å³å¯  https://github.com/guodongxiaren/README/blob/master/emoji.md?tdsourcetag=s_pcqq_aiomsg -->

<!-- - *2022-12* : ğŸ“‘ [Progressive Learning without Forgetting](https://arxiv.org/abs/2211.15215) is online on arXiv. This is a strong extension of [LwF](https://arxiv.org/abs/1606.09282). Enjoy reading it! -->

<!-- - *2023-06* : ğŸ“‘ [VideoComposer](https://arxiv.org/abs/2306.02018), [Progressive Learning without Forgetting](https://arxiv.org/abs/2211.15215) and [Refined Response Distillation](https://arxiv.org/abs/2305.00620) are online on arXiv. Enjoy reading them! -->


- *2025-11* : ğŸ“‘ [UniLumos](https://arxiv.org/abs/2511.01678) and [VideoMAR](https://arxiv.org/abs/2506.14168)  are accepted to **NeurIPS 2025**. Keep going, guys! Media coverage: [æœºå™¨ä¹‹å¿ƒ](https://mp.weixin.qq.com/s/5AMkIj1nSHFckh9aKOicNw). Learn to use it from the community's [interpretation](https://www.youtube.com/watch?v=5ik6tPs6Yq8&t=3s).

- *2025-09* : ğŸ”¥ MM-RoPE in [Lumos-1](https://arxiv.org/abs/2507.08801) is super useful, and similar ideas have been used in [Qwen3-VL](https://github.com/QwenLM/Qwen3-VL). Explanation: [Link](https://www.xiaohongshu.com/discovery/item/68d674b60000000013017625?source=webshare&xhsshare=pc_web&xsec_token=ABuQxDTZq-ttM0uzqPzvwN8DTkb0fD6-Z5oBndn7L8BoA=&xsec_source=pc_share)

- *2025-09* : ğŸ‘‘ I am awarded Zhejiang Provincial **Special Grant** for Postdoctoral Research (**Top 10** in Zhejiang Province).

- *2025-08* : ğŸª„ğŸ¦‰ I will serve as an **Area Chair** for ICLR 2026. 

- *2025-07* : ğŸ¡ We release [Lumos-1](https://arxiv.org/abs/2507.08801), a foundation for autoregresive video generation. Media coverage: [[CVer]](https://mp.weixin.qq.com/s/ejmCHGh5AbxB1mHCO9qpaQ)[[DAMOçŸ©é˜µ]](https://mp.weixin.qq.com/s/ZPUFE1JzgfkAuNzRXPxJiA)[[AIç”Ÿæˆæœªæ¥]](https://mp.weixin.qq.com/s/D508qfcOEjXGZW2F_PN6og)[[PaperWeekly]](https://mp.weixin.qq.com/s/i6JUYP0Dti4i6K95VQioZA)

- *2025-06* : ğŸ“‘ SAMora, [DreamRelation](https://arxiv.org/abs/2503.07602) and [FreeScale](https://arxiv.org/abs/2412.09626)  are accepted to **ICCV 2025**. Well done, guys!

- *2025-03* : ğŸ“‘ [Dual-Arch](https://arxiv.org/abs/2506.03951) and [ZeroFlow](https://arxiv.org/abs/2501.01045) are accepted to **ICML 2025**.

- *2024-12* : ğŸ“‘ [FreeMask](https://freemask-edit.github.io/) and AeroGTO are accepted to **AAAI 2025**. Congrats to Lingling and Pengwei.

- *2024-09* : ğŸ“‘ [EvolveDirector](https://arxiv.org/abs/2410.07133) and [C-Flat](https://arxiv.org/abs/2404.00986) are accepted to **NeurIPS 2024**. Congrats to Rui and Tao.

- *2024-05* : ğŸ“‘ [PAPM](https://openreview.net/forum?id=RtCmp5F9lN) is accepted to **ICML 2024** and [ArchCraft](https://arxiv.org/abs/2404.14829) is accepted to **IJCAI 2024**. I am happy to see that Pengwei and Aojun are able to publish their work at top-tier conferences.

- *2024-02* : ğŸ“‘ [InstructVideo](https://arxiv.org/abs/2312.12490), [DreamVideo](https://arxiv.org/abs/2312.04433) and [TF-T2V](https://arxiv.org/abs/2312.15770) are accepted to **CVPR 2024**. Thrilled to collaborate with them on these promising projects.

- *2024-01* : ğŸ‘‘ I am honored to receive the **Outstanding Research Intern** Award (20 in 1000+ candidates) for my contribution in video generation to Alibaba.

- *2024-01* : ğŸ“‘ [LUM-ViT](https://openreview.net/forum?id=wkbeqr5XhC) is accepted to **ICLR 2024**. Congrats to Lingfeng Liu!

- *2023-09* : ğŸ“‘ [VideoComposer](https://arxiv.org/abs/2306.02018) is accepted to **NeurIPS 2023**.  Thrilled to collaborate with them on this project.

- *2023-08* : ğŸ“‘ [RLIPv2](https://arxiv.org/abs/2308.09351) is accepted to **ICCV 2023**. Code and models are publicly available [here](https://github.com/JacobYuan7/RLIPv2)!

- *2023-08* : ğŸ¡ We release [ModelscopeT2V](https://arxiv.org/abs/2308.06571) (the default T2V in [Diffusers](https://huggingface.co/docs/diffusers/api/pipelines/text_to_video))  and [VideoComposer](https://arxiv.org/abs/2306.02018), two foundations for video generation.

- *2022-09* : ğŸ“‘ [RLIP: Relational Language-Image Pre-training](https://arxiv.org/abs/2209.01814) is accepted to **NeurIPS 2022** as a <span style="color:red"><strong>Spotlight</strong></span> paper (Top 5%). It's my honor to work with [Samuel](https://samuelalbanie.com/) and [Jianwen](https://scholar.google.com/citations?user=uDAkC1kAAAAJ&hl=zh-CN&oi=ao). Btw, the pronunciation of **RLIP** is /'É‘:lÉªp/. 

- *2022-05* : ğŸ“‘ [Elastic Response Distillation](https://openaccess.thecvf.com/content/CVPR2022/html/Feng_Overcoming_Catastrophic_Forgetting_in_Incremental_Object_Detection_via_Elastic_Response_CVPR_2022_paper.html) is accepted to **CVPR 2022**. A great pleasure to work with Tao Feng and Mang Wang.

- *2022-02* : ğŸ‘‘ I am awarded <span style="color:red"><strong>AAAI-22 Scholarship</strong></span>. Acknowledgement to AAAI!

- *2021-12* : ğŸ“‘ [ Object-guided Cross-modal Calibration Network](https://ojs.aaai.org/index.php/AAAI/article/view/20229) is accepted to **AAAI 2022**. A great pleasure to work with Mang Wang.

- *2021-07* : ğŸ“‘ [Spatio-Temporal Dynamic Inference Network](https://openaccess.thecvf.com/content/ICCV2021/html/Yuan_Spatio-Temporal_Dynamic_Inference_Network_for_Group_Activity_Recognition_ICCV_2021_paper.html) is accepted to **ICCV 2021**.

- *2021-03* : ğŸ‘· I start my internship at Alibaba **DAMO Academy**.

- *2020-12* : ğŸ“‘ [Learning Visual Context](https://ojs.aaai.org/index.php/AAAI/article/view/16437) (for Group Activity recognition) is accepted to **AAAI 2021**.


<!-- - *2022.06*: Three papers are accepted by ACM-MM 2022!
- *2022.05*: I join [Sea AI Lab](https://sail.sea.com/) <img src='./images/logo-sea-header-desktop.webp' style='width: 6em;'> as the audio team leader. We are [hiring researchers and engineers](https://career.sea.com/position/427)!
- *2022.04*: Three papers are accepted by IJCAI 2022:
  - SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech, Zhenhui Ye, Zhou Zhao, **Yi Ren**, Fei Wu
  - EditSinger: Zero-Shot Text-Based Singing Voice Editing System with Diverse Prosody Modeling, Lichao Zhang, Zhou Zhao, **Yi Ren**, Liqun Deng
  - FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis, Rongjie Huang, Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu, **Yi Ren**, Zhou Zhao
- *2022.03*: We release [NeuralSVB](https://github.com/MoonInTheRiver/NeuralSVB), the code of our ACL 2022 work (singing voice beautifying). ğŸš§ â›ï¸ ğŸ› ï¸ ğŸ‘· 
- *2022.02*: I release a modern and responsive academic personal [homepage template](https://github.com/RayeRen/acad-homepage.github.io). Welcome to STAR and FORK!
- *2022.02*: ğŸ‰ğŸ‰ Two papers are accepted by ACL 2022:
  - [Revisiting Over-Smoothness in Text to Speech](https://arxiv.org/abs/2202.13066), **Yi Ren**, Xu Tan, Tao Qin, Zhou Zhao, Tie-Yan Liu
  - [Learning the Beauty in Songs: Neural Singing Voice Beautifier](https://arxiv.org/abs/2202.13277), Jinglin Liu, Chengxi Li, **Yi Ren**, Zhiying Zhu, Zhou Zhao \| [![](https://img.shields.io/github/stars/MoonInTheRiver/NeuralSVB?style=social&label=Code+Stars)](https://github.com/MoonInTheRiver/NeuralSVB)
- *2022.02*: ğŸ‰ğŸ‰ My [google scholar](https://scholar.google.com/citations?user=4FA6C0AAAAAJ) citations have exceeded 1000!
- *2022.02*: We public a Non-Autoregressive Text-to-Speech (NAR-TTS) framework [NATSpeech ![](https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social)](https://github.com/NATSpeech/NATSpeech), including official PyTorch implementation of PortaSpeech (NeurIPS 2021) and DiffSpeech (AAAI 2022). ğŸ‰ğŸ‰ It was shown on the [Github Daily Trending List](https://github.motakasoft.com/trending/?d=2022-02-19&l=all) on 19 Feb 2022! -->
